{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leonop/narrativesBERT/blob/main/bertopic_big_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BERTopic on Large Datasets**\n",
        "\n",
        "Due to the modularity of BERTopic, it can also be used on large datasets (> 1_000_000) if we change some of the internal algorithms such that they can scale a bit better.\n",
        "\n",
        "To do so, this example uses Google Colab Pro (T4 with 25GB of VRAM and 26GB of System RAM) and cuML to enable GPU-accelerated machine learning.\n",
        "\n",
        "First, we will need to install some packages:"
      ],
      "metadata": {
        "id": "NHwNgTnKp6R0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swkRsYLdC9YE"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/MaartenGr/BERTopic.git@master\n",
        "\n",
        "!pip install cudf-cu12 dask-cudf-cu12 --extra-index-url=https://pypi.nvidia.com\n",
        "!pip install cuml-cu12 --extra-index-url=https://pypi.nvidia.com\n",
        "!pip install cugraph-cu12 --extra-index-url=https://pypi.nvidia.com\n",
        "!pip install cupy-cuda12x -f https://pip.cupy.dev/aarch64\n",
        "\n",
        "!pip install safetensors\n",
        "!pip install datasets\n",
        "!pip install datashader\n",
        "!pip install adjustText"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, it might happen that you get the `NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968` error, if so make sure to run the following code:\n",
        "\n",
        "```python\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "```"
      ],
      "metadata": {
        "id": "Zs58X6Ifmg2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Optional)** We can mount a personal Google Drive such that we can save the resulting embeddings and models that you create. Although it is not necessary, if you created embeddings and then run into memory errors, it is generally nice to have the embeddings saved somewhere without needing to recalculate them."
      ],
      "metadata": {
        "id": "a8pb6s59m_0x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukzx8_ROU2Hz",
        "outputId": "9388416d-4fed-4092-97f1-a028d081d3b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your CSV file\n",
        "folder_path = '/content/drive/My Drive/Research/TAD_LDA_optimized'\n",
        "file_path = '/content/drive/My Drive/Research/TAD_LDA_optimized/cc_firm_level.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGsO8BShl778"
      },
      "source": [
        "# **Data**\n",
        "\n",
        "To show how BERTopic can be used with large data, we are going to load in some Wikipedia texts. Cohere has fortunately created a dataset split by paragraphs, which allows us to stay within token limit sizes.\n",
        "\n",
        "For this example, we load in 1 million texts from Wikipedia and see if we can extract topics from them."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use chunksize to limit rows number per iteration\n",
        "meta = pd.DataFrame()\n",
        "chunk_reader = pd.read_csv(file_path,chunksize=10000)\n",
        "\n",
        "for chunk in chunk_reader:\n",
        "    chunk[\"publishOn\"] = pd.to_datetime(chunk[\"publishOn\"])\n",
        "    chunk[\"publish_year\"] = (pd.DatetimeIndex(chunk['publishOn']).year)\n",
        "    chunk[\"publish_month\"] = (pd.DatetimeIndex(chunk['publishOn']).month)\n",
        "\n",
        "    # select papers published later than 2013\n",
        "    filtered_chunk = chunk[chunk[\"publish_year\"] <= 2017]\n",
        "    filtered_chunk = filtered_chunk.reset_index()\n",
        "    meta = pd.concat([meta, filtered_chunk], ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596,
          "referenced_widgets": [
            "4499e48731884755b7c39c2e46cecfbf",
            "ff624fd3b628417c8eb3ecd1dac7981e",
            "2ee1aa38888449e6b55df9c5b294867e",
            "918e5f0fd24245b18f192197fe07646e",
            "e58192ad2c664861bc837c4a1a6898a1",
            "a9b9a0c5ad034728a0f420763f49560f",
            "ba079663caef4048bcd2c321ec6d284b",
            "f86aba8277404fd8b85d34132710b6fd",
            "ddbc57eca23b414e82668bf27c4849e1",
            "df24db180bf04123b263317d63aada14",
            "00a756401544421cb39a7b662674299d",
            "e7da4d01492842278bf1d4da55f7149f",
            "15bf81a343784849a3e58ff2e9622472",
            "5170da168feb4566aca227bd06386e20",
            "fab34a06b03b46779c47b211b1a916d0",
            "3dc490f14be2474eb06fb9f79067982a",
            "dd695b7dd20645de8a6906d805dcf329",
            "66a6089cb7ce4716a7844529819d6645",
            "6ac07fb0172e4a29b8f89754d0f6d840",
            "1773e0f9dac94042b1d009674ba70328",
            "00f24a011b1443319e2a1d4991770148",
            "a2c4827b01ab4c3081ccfd614eeea229"
          ]
        },
        "id": "Dd-ANJvnxph4",
        "outputId": "fa866336-0e51-4d7f-e5c7-00557cfebe68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/3.39k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4499e48731884755b7c39c2e46cecfbf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/3.87k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7da4d01492842278bf1d4da55f7149f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b29867894837>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Extract 1 millions records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cohere/wikipedia-22-12\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreaming\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"1_000_000\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2605\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2606\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2607\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2608\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   2275\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2276\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2277\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   2278\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2279\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1894\u001b[0m                     \u001b[0mdynamic_modules_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdynamic_modules_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1895\u001b[0m                     \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1896\u001b[0;31m                 ).get_module()\n\u001b[0m\u001b[1;32m   1897\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1898\u001b[0m                 return HubDatasetModuleFactoryWithoutScript(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mget_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1530\u001b[0m         )\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportable_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0mtrust_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve_trust_remote_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m                 _create_importable_file(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mresolve_trust_remote_code\u001b[0;34m(trust_remote_code, repo_id)\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malarm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTIME_OUT_REMOTE_CODE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mtrust_remote_code\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                     answer = input(\n\u001b[0m\u001b[1;32m    122\u001b[0m                         \u001b[0;34mf\"The repository for {repo_id} contains custom code which must be executed to correctly \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                         \u001b[0;34mf\"load the dataset. You can inspect the repository content at https://hf.co/datasets/{repo_id}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Basic Example**\n",
        "\n",
        "Now that we have our data, we can start with a basic example. This example shows the minimum steps necessary for training a BERTopic model on large datasets. Do note though that memory errors are still possible when tweaking parameters. After this section, some tips and tricks will be mentioned to demonstrate how we can further reduce memory or be more efficient with our training process."
      ],
      "metadata": {
        "id": "kWxZKbi_p8BW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuJA4WTxmDqs"
      },
      "source": [
        "## Embeddings\n",
        "\n",
        "Next, we are going to pre-calculate the embeddings as input for our BERTopic model. The reason for doing this is that this input step can take quite some time to compute. If we pre-calculate them and save the resulting embeddings, we can skip over this step when we are iterating over our model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Create embeddings\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "embeddings = model.encode(docs, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "PeUXF7nOxDjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can save the resulting embeddings as follows:"
      ],
      "metadata": {
        "id": "kcOMYUNdnuVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "with open('/content/drive/MyDrive/embeddings.npy', 'wb') as f:\n",
        "    np.save(f, embeddings)"
      ],
      "metadata": {
        "id": "jGJge6cqxlLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and then load the embeddings as follows:"
      ],
      "metadata": {
        "id": "zD4EBs6dxALj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = np.load('/content/drive/MyDrive/embeddings.npy')"
      ],
      "metadata": {
        "id": "bvo2URBXOy8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Vocab\n",
        "\n",
        "Before we can start with training our model, there is still something that we can do to reduce the necessary memory. We can prepare our vocabulary beforehand such that the tokenizer does not need to do the calculations itself.\n",
        "\n",
        "Below, we are essentially creating a vocabulary of words in our dataset and parsing them such that they need to appear at least 15 times in our data.\n",
        "\n",
        "With sufficient RAM, we could skip over this step but I have found this often helps in reducing the necessary RAM.\n",
        "\n",
        "**Note**: If you are using a custom CountVectorizer, then it is advised to use that instead of the default CountVectorizer to build the tokenizer."
      ],
      "metadata": {
        "id": "-bX8BEd7zph0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Extract vocab to be used in BERTopic\n",
        "vocab = collections.Counter()\n",
        "tokenizer = CountVectorizer().build_tokenizer()\n",
        "for doc in tqdm(docs):\n",
        "  vocab.update(tokenizer(doc))\n",
        "vocab = [word for word, frequency in vocab.items() if frequency >= 15]; len(vocab)"
      ],
      "metadata": {
        "id": "SmxBX5UIzl_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train BERTopic\n",
        "\n",
        "Finally, we can train our BERTopic model. We select the same sentence-transformer model as we did before. Moreover, we select cuML's HDBSCAN and UMAP models. These are GPU-accelerated versions of HDBSCAN and UMAP and allow us to speed training on such a large dataset."
      ],
      "metadata": {
        "id": "yeB091ZHzyRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cuml.manifold import UMAP\n",
        "from cuml.cluster import HDBSCAN\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# Prepare sub-models\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "umap_model = UMAP(n_components=5, n_neighbors=50, random_state=42, metric=\"cosine\", verbose=True)\n",
        "hdbscan_model = HDBSCAN(min_samples=20, gen_min_span_tree=True, prediction_data=False, min_cluster_size=20, verbose=True)\n",
        "vectorizer_model = CountVectorizer(vocabulary=vocab, stop_words=\"english\")\n",
        "\n",
        "# Fit BERTopic without actually performing any clustering\n",
        "topic_model= BERTopic(\n",
        "        embedding_model=embedding_model,\n",
        "        umap_model=umap_model,\n",
        "        hdbscan_model=hdbscan_model,\n",
        "        vectorizer_model=vectorizer_model,\n",
        "        verbose=True\n",
        ").fit(docs, embeddings=embeddings)"
      ],
      "metadata": {
        "id": "OvnyNuYJzy_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and that is it! We can inspect the topics as follows:"
      ],
      "metadata": {
        "id": "3tJJAjYjpmMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic_info()"
      ],
      "metadata": {
        "id": "uMAjGXEwpx3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, it would be advised to save the model as follows:"
      ],
      "metadata": {
        "id": "BbMq1EOB8dol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.save(\n",
        "    path='/content/drive/MyDrive/model_dir',\n",
        "    serialization=\"safetensors\",\n",
        "    save_ctfidf=True,\n",
        "    save_embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")"
      ],
      "metadata": {
        "id": "OORSPVUFxIM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Advanced Tips & Tricks**\n",
        "\n",
        "There are a number of advanced tips and tricks that you can use to make it a bit easier and more flexible to train your model at large datasets."
      ],
      "metadata": {
        "id": "pD45EVeNowEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UMAP\n",
        "\n",
        "Just like we pre-computed the embeddings, we can pre-reduce the dimensionality of the embeddings with cuML's UMAP and use those.\n",
        "\n",
        "This saves quite a bit of time since we need to reduce embeddings everytime we run BERTopic.\n",
        "\n",
        "**NOTE**: Saving those embeddings, as we did before, is generally advised."
      ],
      "metadata": {
        "id": "HMVTNjkMzTSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cuml.manifold import UMAP\n",
        "\n",
        "# Train model and reduce dimensionality of embeddings\n",
        "umap_model = UMAP(n_components=5, n_neighbors=15, random_state=42, metric=\"cosine\", verbose=True)\n",
        "reduced_embeddings = umap_model.fit_transform(embeddings)"
      ],
      "metadata": {
        "id": "3OSVNctDzT_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we can also prepare the UMAP-reduced embeddings for visualizing documents in 2D:"
      ],
      "metadata": {
        "id": "j9f-f0OtAZuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cuml.manifold import UMAP\n",
        "\n",
        "# Train model and reduce dimensionality of embeddings\n",
        "umap_model = UMAP(n_components=2, n_neighbors=15, random_state=42, metric=\"cosine\", verbose=True)\n",
        "reduced_embeddings_2d = umap_model.fit_transform(embeddings)"
      ],
      "metadata": {
        "id": "jRTwZ-PLxglI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HDBSCAN\n",
        "\n",
        "We can even pre-compute the clusters with HDBSCAN and feed them to BERTopic to perform [manual topic modeling](https://maartengr.github.io/BERTopic/getting_started/manual/manual.html). Manual topic modeling with BERTopic means that if we already have labels of topics, we can use those to label them using a variety of [representation models](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html).\n",
        "\n",
        "BERTopic can still perform inference since inference is done on topic embeddings that are created during `fit`."
      ],
      "metadata": {
        "id": "Gt-zQjd6qltP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cuml.cluster import HDBSCAN\n",
        "\n",
        "# Find clusters of semantically similar documents\n",
        "hdbscan_model = HDBSCAN(min_samples=30, gen_min_span_tree=True, prediction_data=False, min_cluster_size=30, verbose=True)\n",
        "clusters = hdbscan_model.fit(reduced_embeddings).labels_"
      ],
      "metadata": {
        "id": "oGQnu6lO9SrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manual BERTopic\n",
        "\n",
        "When we have pre-computed labels, we can pass them instead of having to calculate the clusters ourselves. This allows us to skip over the embedding, dimensionality, and clustering steps!\n",
        "\n",
        "There is one more trick though that we are going to use. We could directly give BERTopic the reduced embeddings but BERTopic will use those to create topic vectors which is not ideal. Instead, we are giving the model the full embeddings and create a custom dimensionality reduction class that will return the reduced embeddings. This gives us the best of both worlds!"
      ],
      "metadata": {
        "id": "Ls2Q-iccGs7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cuml.cluster import HDBSCAN\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from bertopic import BERTopic\n",
        "from bertopic.cluster import BaseCluster\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "\n",
        "\n",
        "class Dimensionality:\n",
        "  \"\"\" Use this for pre-calculated reduced embeddings \"\"\"\n",
        "  def __init__(self, reduced_embeddings):\n",
        "    self.reduced_embeddings = reduced_embeddings\n",
        "\n",
        "  def fit(self, X):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    return self.reduced_embeddings\n",
        "\n",
        "\n",
        "# Prepare sub-models\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "umap_model = Dimensionality(reduced_embeddings)\n",
        "hdbscan_model = BaseCluster()\n",
        "vectorizer_model = CountVectorizer(vocabulary=vocab, stop_words=\"english\")\n",
        "representation_model = KeyBERTInspired()\n",
        "\n",
        "# Fit BERTopic without actually performing any clustering\n",
        "topic_model= BERTopic(\n",
        "        embedding_model=embedding_model,\n",
        "        umap_model=umap_model,\n",
        "        hdbscan_model=hdbscan_model,\n",
        "        vectorizer_model=vectorizer_model,\n",
        "        representation_model=representation_model,\n",
        "        verbose=True\n",
        ").fit(docs, embeddings=embeddings, y=clusters)"
      ],
      "metadata": {
        "id": "IpDyS8MLGOTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic_info()"
      ],
      "metadata": {
        "id": "c4X46XYpPdo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Documents"
      ],
      "metadata": {
        "id": "2Affr9_x9s0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "# Define colors for the visualization to iterate over\n",
        "colors = itertools.cycle(['#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#ffffff', '#000000'])\n",
        "color_key = {str(topic): next(colors) for topic in set(topic_model.topics_) if topic != -1}\n",
        "\n",
        "# Prepare dataframe and ignore outliers\n",
        "df = pd.DataFrame({\"x\": reduced_embeddings_2d[:, 0], \"y\": reduced_embeddings_2d[:, 1], \"Topic\": [str(t) for t in topic_model.topics_]})\n",
        "df[\"Length\"] = [len(doc) for doc in docs]\n",
        "df = df.loc[df.Topic != \"-1\"]\n",
        "df = df.loc[(df.y > -10) & (df.y < 10) & (df.x < 10) & (df.x > -10), :]\n",
        "df[\"Topic\"] = df[\"Topic\"].astype(\"category\")\n",
        "\n",
        "# Get centroids of clusters\n",
        "mean_df = df.groupby(\"Topic\").mean().reset_index()\n",
        "mean_df.Topic = mean_df.Topic.astype(int)\n",
        "mean_df = mean_df.sort_values(\"Topic\")"
      ],
      "metadata": {
        "id": "XupETxy89uqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from adjustText import adjust_text\n",
        "import matplotlib.patheffects as pe\n",
        "\n",
        "fig = plt.figure(figsize=(16, 16))\n",
        "sns.scatterplot(data=df, x='x', y='y', c=df['Topic'].map(color_key), alpha=0.4, sizes=(0.4, 10), size=\"Length\")\n",
        "\n",
        "# Annotate top 50 topics\n",
        "texts, xs, ys = [], [], []\n",
        "for row in mean_df.iterrows():\n",
        "  topic = row[1][\"Topic\"]\n",
        "  name = \" - \".join(list(zip(*topic_model.get_topic(int(topic))))[0][:3])\n",
        "\n",
        "  if int(topic) <= 50:\n",
        "    xs.append(row[1][\"x\"])\n",
        "    ys.append(row[1][\"y\"])\n",
        "    texts.append(plt.text(row[1][\"x\"], row[1][\"y\"], name, size=10, ha=\"center\", color=color_key[str(int(topic))],\n",
        "                          path_effects=[pe.withStroke(linewidth=0.5, foreground=\"black\")]))\n",
        "\n",
        "# Adjust annotations such that they do not overlap\n",
        "adjust_text(texts, x=xs, y=ys, time_lim=1, force_text=(0.01, 0.02), force_static=(0.01, 0.02), force_pull=(0.5, 0.5))\n",
        "plt.show()\n",
        "# plt.savefig(\"visualization2.png\", dpi=600)"
      ],
      "metadata": {
        "id": "DIv6fRecTvTw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4499e48731884755b7c39c2e46cecfbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff624fd3b628417c8eb3ecd1dac7981e",
              "IPY_MODEL_2ee1aa38888449e6b55df9c5b294867e",
              "IPY_MODEL_918e5f0fd24245b18f192197fe07646e"
            ],
            "layout": "IPY_MODEL_e58192ad2c664861bc837c4a1a6898a1"
          }
        },
        "ff624fd3b628417c8eb3ecd1dac7981e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9b9a0c5ad034728a0f420763f49560f",
            "placeholder": "​",
            "style": "IPY_MODEL_ba079663caef4048bcd2c321ec6d284b",
            "value": "Downloading builder script: 100%"
          }
        },
        "2ee1aa38888449e6b55df9c5b294867e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f86aba8277404fd8b85d34132710b6fd",
            "max": 3388,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ddbc57eca23b414e82668bf27c4849e1",
            "value": 3388
          }
        },
        "918e5f0fd24245b18f192197fe07646e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df24db180bf04123b263317d63aada14",
            "placeholder": "​",
            "style": "IPY_MODEL_00a756401544421cb39a7b662674299d",
            "value": " 3.39k/3.39k [00:00&lt;00:00, 29.2kB/s]"
          }
        },
        "e58192ad2c664861bc837c4a1a6898a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9b9a0c5ad034728a0f420763f49560f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba079663caef4048bcd2c321ec6d284b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f86aba8277404fd8b85d34132710b6fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddbc57eca23b414e82668bf27c4849e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df24db180bf04123b263317d63aada14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00a756401544421cb39a7b662674299d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7da4d01492842278bf1d4da55f7149f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15bf81a343784849a3e58ff2e9622472",
              "IPY_MODEL_5170da168feb4566aca227bd06386e20",
              "IPY_MODEL_fab34a06b03b46779c47b211b1a916d0"
            ],
            "layout": "IPY_MODEL_3dc490f14be2474eb06fb9f79067982a"
          }
        },
        "15bf81a343784849a3e58ff2e9622472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd695b7dd20645de8a6906d805dcf329",
            "placeholder": "​",
            "style": "IPY_MODEL_66a6089cb7ce4716a7844529819d6645",
            "value": "Downloading readme: 100%"
          }
        },
        "5170da168feb4566aca227bd06386e20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ac07fb0172e4a29b8f89754d0f6d840",
            "max": 3872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1773e0f9dac94042b1d009674ba70328",
            "value": 3872
          }
        },
        "fab34a06b03b46779c47b211b1a916d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00f24a011b1443319e2a1d4991770148",
            "placeholder": "​",
            "style": "IPY_MODEL_a2c4827b01ab4c3081ccfd614eeea229",
            "value": " 3.87k/3.87k [00:00&lt;00:00, 33.6kB/s]"
          }
        },
        "3dc490f14be2474eb06fb9f79067982a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd695b7dd20645de8a6906d805dcf329": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66a6089cb7ce4716a7844529819d6645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ac07fb0172e4a29b8f89754d0f6d840": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1773e0f9dac94042b1d009674ba70328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00f24a011b1443319e2a1d4991770148": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2c4827b01ab4c3081ccfd614eeea229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}